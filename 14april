1. Preprocess the dataset by handling missing values, encoding categorical variables, and scaling the
numerical features if necessary.

1)Handling Missing Values: Identify the columns with missing values in the dataset. Decide on a strategy to handle missing values, such as imputation (filling missing values with a reasonable estimate) or removal of rows/columns with missing values. For numerical features, common imputation methods include mean, median, or using machine learning algorithms to predict missing values based on other features. For categorical features, you can either add a new category to represent missing values or use the most frequent category. Encoding Categorical Variables:

2)Categorical variables need to be converted into numerical format so that machine learning algorithms can work with them. One-hot encoding: For nominal categorical variables (categories with no ordinal relationship), you can use one-hot encoding to create binary columns for each category, where 1 indicates the presence of that category and 0 indicates the absence. Label encoding: For ordinal categorical variables (categories with a specific order), you can use label encoding to map categories to integers.

3)Scaling Numerical Features: If your numerical features have different scales, it's often beneficial to scale them to a similar range. Common scaling methods include Min-Max scaling (scaling features to a specified range, usually [0, 1]) and Standardization (scaling features to have mean=0 and standard deviation=1). Scaling is not always necessary for all machine learning algorithms, but it can improve the performance of some algorithms that are sensitive to feature scales.

import pandas as pd
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import OneHotEncoder, StandardScaler

# Assuming 'data' is your dataset loaded into a pandas DataFrame.

# Step 1: Handling Missing Values
# Identify columns with missing values
columns_with_missing_values =data.columns[data.isnull().any()].tolist()

# Decide on a strategy for imputation (e.g., using mean for numerical features)
imputer = SimpleImputer(strategy='mean')
data[columns_with_missing_values] = imputer.fit_transform(data[columns_with_missing_values])

# Step 2: Encoding Categorical Variables
# Identify categorical columns
categorical_columns = data.select_dtypes(include=['object']).columns.tolist()

# One-hot encoding for categorical columns
encoder = OneHotEncoder()
encoded_features = encoder.fit_transform(data[categorical_columns]).toarray()
encoded_df = pd.DataFrame(encoded_features, columns=encoder.get_feature_names(categorical_columns))
data = pd.concat([data, encoded_df], axis=1)
data.drop(columns=categorical_columns, inplace=True)

# Step 3: Scaling Numerical Features (if necessary)
# Identify numerical columns
numerical_columns = data.select_dtypes(include=['float64', 'int64']).columns.tolist()

# Standardization for numerical columns
scaler = StandardScaler()
data[numerical_columns] = scaler.fit_transform(data[numerical_columns])

# Now, the 'data' DataFrame should be preprocessed and ready for use in machine learning models.
---------------------------------------------------------------------------
NameError                                 Traceback (most recent call last)
Cell In[2], line 9
      3 from sklearn.preprocessing import OneHotEncoder, StandardScaler
      5 # Assuming 'data' is your dataset loaded into a pandas DataFrame.
      6 
      7 # Step 1: Handling Missing Values
      8 # Identify columns with missing values
----> 9 columns_with_missing_values =data.columns[data.isnull().any()].tolist()
     11 # Decide on a strategy for imputation (e.g., using mean for numerical features)
     12 imputer = SimpleImputer(strategy='mean')

NameError: name 'data' is not defined
Q2. Split the dataset into a training set (70%) and a test set (30%).
from sklearn.model_selection import train_test_split

# Assuming 'data' is your preprocessed DataFrame and 'target' is the target variable column

# Splitting the dataset into training (70%) and test (30%) sets
train_data, test_data, train_target, test_target = train_test_split(
    data.drop(columns=['target']),  # Drop the target variable column from the features
    data['target'],  # Target variable column
    test_size=0.3,   # Proportion of test set (30%)
    random_state=42  # Setting a random seed for reproducibility
)

# Now, you have the following datasets ready:
# train_data: Training set features (70% of the original data)
# test_data: Test set features (30% of the original data)
# train_target: Training set target variable
# test_target: Test set target variable
Q3. Train a random forest classifier on the training set using 100 trees and a maximum depth of 10 for each
tree. Use the default values for other hyperparameters.

from sklearn.ensemble import RandomForestClassifier

# Assuming you have already split the dataset into train_data, test_data, train_target, and test_target

# Create the RandomForestClassifier with the specified hyperparameters
rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)

# Train the model on the training set
rf_classifier.fit(train_data, train_target)

# Predict on the test set
predictions = rf_classifier.predict(test_data)

# Now, you have trained the random forest classifier and made predictions on the test set.
# You can proceed to evaluate the performance of the model using various metrics, such as accuracy, precision, recall, etc.
Q4. Evaluate the performance of the model on the test set using accuracy, precision, recall, and F1 score.
To evaluate the performance of the trained random forest classifier on the test set using accuracy, precision, recall, and F1 score, you can use the metrics available in the scikit-learn library in Python. Here's how you can calculate these metrics:
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming you have already trained the random forest classifier and made predictions on the test set
# rf_classifier: Trained RandomForestClassifier
# test_data: Test set features
# test_target: True labels for the test set

# Make predictions on the test set
predictions = rf_classifier.predict(test_data)

# Calculate accuracy
accuracy = accuracy_score(test_target, predictions)

# Calculate precision
precision = precision_score(test_target, predictions)

# Calculate recall
recall = recall_score(test_target, predictions)

# Calculate F1 score
f1 = f1_score(test_target, predictions)

# Print the evaluation metrics
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
The code above will print the values of accuracy, precision, recall, and F1 score for the random forest classifier's performance on the test set. These metrics provide insights into different aspects of the model's performance:

Accuracy measures the proportion of correctly classified instances out of all instances in the test set. Precision measures the proportion of true positive predictions out of all positive predictions made by the model. Recall (also known as sensitivity) measures the proportion of true positive predictions out of all actual positive instances in the test set. F1 score is the harmonic mean of precision and recall, providing a balanced evaluation metric for imbalanced datasets.

Q5. Use the feature importance scores to identify the top 5 most important features in predicting heart
disease risk. Visualise the feature importances using a bar chart.

Let's identify the top 5 most important features in predicting heart disease risk using the feature importances from the random forest classifier and visualize them with a bar chart. Here's the correct way to do it:

import matplotlib.pyplot as plt

# Assuming you have already trained the random forest classifier and stored it in rf_classifier

# Get feature importances from the trained model
feature_importances = rf_classifier.feature_importances_

# Create a DataFrame to associate feature names with their importances
feature_importance_df = pd.DataFrame({'Feature': train_data.columns, 'Importance': feature_importances})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Select the top 5 most important features
top_5_features = feature_importance_df.head(5)

# Visualize the feature importances using a bar chart
plt.figure(figsize=(10, 6))
plt.bar(top_5_features['Feature'], top_5_features['Importance'])
plt.xlabel('Feature')
plt.ylabel('Importance')
plt.title('Top 5 Most Important Features for Heart Disease Risk Prediction')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()
In this code, we are assuming that train_data is the DataFrame containing the features used for training the model. The train_data.columns attribute provides the feature names. We then create a DataFrame called feature_importance_df to associate each feature's name with its corresponding importance score. We sort this DataFrame in descending order based on importance and select the top 5 features. Finally, we visualize the top 5 most important features using a bar chart.

Q6. Tune the hyperparameters of the random forest classifier using grid search or random search. Try
different values of the number of trees, maximum depth, minimum samples split, and minimum samples leaf. Use 5-fold cross-validation to evaluate the performance of each set of hyperparameters.

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV

# Assuming you have already split the dataset into train_data, test_data, train_target, and test_target

# Create the RandomForestClassifier with default hyperparameters
rf_classifier = RandomForestClassifier(random_state=42)

# Define the hyperparameter grid or distribution for the search
param_grid = {
    'n_estimators': [50, 100, 150],     # Different values for the number of trees
    'max_depth': [5, 10, 15],           # Different values for the maximum depth
    'min_samples_split': [2, 5, 10],    # Different values for minimum samples split
    'min_samples_leaf': [1, 2, 4]       # Different values for minimum samples leaf
}

# Create a GridSearchCV or RandomizedSearchCV object
# For Grid Search:
# search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=5)

# For Random Search:
search = RandomizedSearchCV(estimator=rf_classifier, param_distributions=param_grid, n_iter=10, cv=5, random_state=42)

# Perform the search and fit the model to find the best hyperparameters
search.fit(train_data, train_target)

# Get the best hyperparameters and the corresponding best model
best_params = search.best_params_
best_rf_classifier = search.best_estimator_

# Evaluate the best model on the test set
best_predictions = best_rf_classifier.predict(test_data)

# Calculate the performance metrics on the test set
accuracy = accuracy_score(test_target, best_predictions)
precision = precision_score(test_target, best_predictions)
recall = recall_score(test_target, best_predictions)
f1 = f1_score(test_target, best_predictions)

# Print the evaluation metrics and the best hyperparameters
print("Best Hyperparameters:", best_params)
print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)
Q7. Report the best set of hyperparameters found by the search and the corresponding performance
metrics. Compare the performance of the tuned model with the default model.

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Assuming you have already trained the default random forest classifier and stored it in rf_classifier
# Also, you have performed RandomizedSearchCV and obtained the best hyperparameters and the best model

# Evaluate the default model on the test set
default_predictions = rf_classifier.predict(test_data)
default_accuracy = accuracy_score(test_target, default_predictions)
default_precision = precision_score(test_target, default_predictions)
default_recall = recall_score(test_target, default_predictions)
default_f1 = f1_score(test_target, default_predictions)

# Evaluate the best model on the test set
best_predictions = best_rf_classifier.predict(test_data)
best_accuracy = accuracy_score(test_target, best_predictions)
best_precision = precision_score(test_target, best_predictions)
best_recall = recall_score(test_target, best_predictions)
best_f1 = f1_score(test_target, best_predictions)

# Print the evaluation metrics and the best hyperparameters
print("Best Hyperparameters:", best_params)

print("Default Model Performance:")
print("Accuracy:", default_accuracy)
print("Precision:", default_precision)
print("Recall:", default_recall)
print("F1 Score:", default_f1)

print("\nTuned Model Performance:")
print("Accuracy:", best_accuracy)
print("Precision:", best_precision)
print("Recall:", best_recall)
print("F1 Score:", best_f1)
Q8. Interpret the model by analysing the decision boundaries of the random forest classifier. Plot the
decision boundaries on a scatter plot of two of the most important features. Discuss the insights and limitations of the model for predicting heart disease risk.

Interpreting the decision boundaries of a random forest classifier can provide insights into how the model makes predictions. However, since random forests are ensemble models composed of multiple decision trees, visualizing the decision boundaries directly can be challenging. Instead, we can use the model's predictions to create a contour plot that approximates the decision boundaries on a scatter plot of two of the most important features.

Let's assume you have identified the two most important features based on the feature importances obtained during the model training. For simplicity, we'll use these two features to create the scatter plot and overlay the decision boundaries. Keep in mind that decision boundaries in higher-dimensional spaces can be more complex and may not be accurately represented in two-dimensional scatter plots.

import numpy as np

# Assuming you have already trained the random forest classifier and identified the two most important features
# Let's assume the two most important features are 'feature1' and 'feature2'

# Generate data points for the scatter plot within the range of the two features
# You can modify this based on your data distribution
feature1_range = np.linspace(train_data['feature1'].min(), train_data['feature1'].max(), 100)
feature2_range = np.linspace(train_data['feature2'].min(), train_data['feature2'].max(), 100)
feature1_values, feature2_values = np.meshgrid(feature1_range, feature2_range)

# Create input data for the model by combining the two features
input_data = np.column_stack((feature1_values.ravel(), feature2_values.ravel()))

# Get the model predictions for the input data
predictions = best_rf_classifier.predict(input_data)

# Reshape the predictions to the shape of the meshgrid
predictions = predictions.reshape(feature1_values.shape)

# Plot the decision boundaries on the scatter plot
plt.figure(figsize=(10, 6))
plt.scatter(train_data['feature1'], train_data['feature2'], c=train_target, cmap='coolwarm', alpha=0.6)
plt.contourf(feature1_values, feature2_values, predictions, alpha=0.3, cmap='coolwarm')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.title('Decision Boundaries of Random Forest Classifier')
plt.colorbar(label='Predicted Class')
plt.tight_layout()
plt.show()
Interpreting the insights and limitations of the model for predicting heart disease risk from this visualization can be challenging. Some insights that can be derived include:

The decision boundaries provide a rough indication of how the model separates different classes based on the two most important features. The regions enclosed by the same decision boundary color represent areas where the model predicts the same class. Decision boundaries are non-linear due to the random forest's ensemble nature, allowing the model to capture complex relationships between features and the target variable. However, it's important to note some limitations:

This visualization only shows the decision boundaries for two features, while the model's decision-making involves multiple features. The full decision boundaries in higher-dimensional feature spaces can be much more complex. The model's performance is not solely determined by these two features. Other features might also play critical roles in predicting heart disease risk. The data distribution in the plot may not be representative of the entire dataset, and the visualization does not capture the model's performance on unseen data.

 
